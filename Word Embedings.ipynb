{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module nltk is natural language processing toolkit which provides certain requirements. From this we would import stopwords in english and corpus, here brown, which contains 500 samples joined together which will work as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\deshw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\deshw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import stopwords,brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import scipy.cluster.vq as mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=set(stopwords.words('english'))\n",
    "corpus=[str(word).lower() for word in brown.words() if (word.lower() not in sw) & (len(word)>1) & (word.isalnum())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading liabraries and data we have already defined our corpus, the word stream. Now its time for our vocabulary and context.<br>\n",
    "<br>\n",
    "vocab  : This defines the size of our vocabulary. How many words we want to take into consideration.<br>\n",
    "<br>\n",
    "context: These can be regarded as features which contain probablity of frequent words occuring in the context of vocabulary words. In short, considering context similarity could be derived from vocabulary words to only those words which are frequent. These are in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=len(corpus)\n",
    "count={}\n",
    "words=[]\n",
    "for word in corpus:\n",
    "    count[word]=count.get(word,0)+1\n",
    "    if word not in words:\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[word for word in words if count[word]>19]\n",
    "context=[word for word in words if count[word]>99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For less computation in future we could save the the two in pickle. Pickling refers to serializing the data and storing in the form of bytes stream, i.e. in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open('embeddings.pickle','wb')\n",
    "string='The file contains vocabulary words, context words and the word distribution matrix repectively'\n",
    "pickle.dump(string,file)\n",
    "pickle.dump(vocab,file)\n",
    "pickle.dump(context,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process of embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the words in the context of vocab words with a user defined window. Window may be the number of words that should be taken in the context of vocab words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_counts(window=2):\n",
    "    c={}\n",
    "    for w0 in vocab:\n",
    "        c[w0]={}\n",
    "        \n",
    "    for i in range(window,N-window):\n",
    "        w0=corpus[i]\n",
    "        if w0 in vocab:\n",
    "            for j in list(range(i-window,i))+list(range(i+1,i+window+1)):\n",
    "                w=corpus[j]\n",
    "                if w in context:\n",
    "                    c[w0][w]=c[w0].get(w,0)+1\n",
    "    return c            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we come to the second step, calculation of probablity of a word to be in the context of vocab word. Which is defined by the ratio of the number of times the word appeared in the surrounding of the vocab words to the number of words in the context of that particular vocab word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurence(vector):\n",
    "    co={}\n",
    "    for w0 in vector:\n",
    "        co[w0]={}\n",
    "        add=sum(vector[w0].values())\n",
    "        if add>0:\n",
    "            for w in vector[w0]:\n",
    "                co[w0][w]=float(vector[w0][w]/add)\n",
    "    return co  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate the probablity of all vocab words. The ratio of the number of times a word appeared in context of ANY word to the TOTAL number of words considered in the contexts of vocab words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_prob(vector):\n",
    "    con_prob={}\n",
    "    con_tot={}\n",
    "    total=0\n",
    "    for w0 in vector:\n",
    "        for w in vector[w0]:\n",
    "            total=total+vector[w0][w]\n",
    "            con_tot[w]=con_tot.get(w,0)+vector[w0][w]\n",
    "    for w in con_tot:\n",
    "        con_prob[w]=con_tot[w]/total\n",
    "    return con_prob        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last one is to define the resultant probablity. Here we take 0 for those words which are not in the context and those which have a negative probablity(the words which appeared more in the context of other words than the word referring to). Log of the probablity is taken. Thus a probablity distribution is defined for every vocab word in reference to every context word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_distribution(counts):\n",
    "    co=co_occurence(counts)\n",
    "    probab=context_prob(co)\n",
    "    n,d=len(vocab),len(context)\n",
    "    distribution=np.zeros((n,d))\n",
    "\n",
    "    for i in range(n):\n",
    "        w0=vocab[i]\n",
    "        for w in co[w0].keys():\n",
    "            j=context.index(w)\n",
    "            distribution[i,j]=max(0.0,np.log(co[w0][w])-np.log(probab[w]))\n",
    "    return distribution     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling functons for final probablity distribution over context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=context_counts()\n",
    "dis=word_distribution(counts)\n",
    "pickle.dump(dis,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the features in the given method contains much features which can be narrowed down to specific ones by using principal component analysis which steers the data into new directions, natural directoins or cordinate system of the matrix, which contains much of the variability in the principal components only by using Eigen vectors and Eigen values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains vocabulary words, context words and the word distribution matrix repectively\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "file=open('embeddings.pickle','rb')\n",
    "print(pickle.load(file))\n",
    "vocab=pickle.load(file)\n",
    "context=pickle.load(file)\n",
    "\n",
    "model=PCA(100)\n",
    "distribution=model.fit_transform(pickle.load(file))\n",
    "for i in range(len(vocab)):\n",
    "    distribution[i]=distribution[i]/np.linalg.norm(distribution[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extract(word):\n",
    "    if word in vocab:\n",
    "        diff=np.zeros(len(vocab))\n",
    "        i=vocab.index(word)\n",
    "        for j in range(len(vocab)):\n",
    "            diff[j]=np.linalg.norm(distribution[i]-distribution[j])\n",
    "        diff[i]=1000\n",
    "        dist=np.argmin(diff)\n",
    "        return vocab[dist]\n",
    "        \n",
    "    else:\n",
    "        print('Word not found!')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experience'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_extract('human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using bag-of-words, the process converts data into a vector where each row refers to a line and each row corresponds to the words in the vocab. If they are present, it gives out 1 else 0, a binary representation.<br>\n",
    "While in case of word embedding it gives out a distributed representation, a probablity distribution. Every word is represented as a vector. And two similar words have little difference between them, they are directed to a similar direction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
