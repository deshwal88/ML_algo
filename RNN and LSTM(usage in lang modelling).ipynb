{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import tensorflow.keras as keras\n",
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we specify a few things:<br>\n",
    "1. number of LSTM layers<br>\n",
    "2. number of time steps(words in a sentence)<br>\n",
    "3. batch size(no of sentences ina batch)<br>\n",
    "4. number of hidden nodes in the two layers<br>\n",
    "5. vocabulary size<br>\n",
    "6. vector size for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.0\n",
    "max_grad_norm = 5\n",
    "num_layers = 2\n",
    "num_steps = 20\n",
    "hidden_1 = 256\n",
    "hidden_2 = 128\n",
    "\n",
    "batch_size = 30\n",
    "vocab_size = 10000\n",
    "embedding_vec_size= 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and parsing content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reading we will user reader.py file already provided. It contains certain functions for parsing data and dividing the data into batches, sequences(time steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='../Datasets ML/simple-examples/data'\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_word(id_list):\n",
    "    words=[]\n",
    "    for id_ in id_list:\n",
    "        for word,w_id in word_to_id.items():\n",
    "            if w_id==id_:\n",
    "                words.append(word)\n",
    "                break\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance lets take single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iterator_=reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_batch=_iterator_.__next__()\n",
    "inp=first_batch[0]\n",
    "tar=first_batch[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer basically converts each word to a vector of given dimensions alongwith specifying the vocabulary size.<br>\n",
    "Further the layer trains the vectors for each word by updating probablity values. In short it is a Word2vec layer which trains itself through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=keras.layers.Embedding(input_dim=vocab_size,output_dim=embedding_vec_size,\\\n",
    "                                       batch_input_shape=(batch_size,num_steps),input_length=num_steps,trainable=True,\\\n",
    "                                       name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_1=embedding_layer(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing RNN Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the layers trainable means we do not require to specify gradient Tape to watch for trainable variables, it does it automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=keras.layers.LSTMCell(hidden_1)\n",
    "l2=keras.layers.LSTMCell(hidden_2)\n",
    "stacked_rnn=keras.layers.StackedRNNCells([l1,l2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer=keras.layers.RNN(stacked_rnn,[batch_size,num_steps],trainable=True,stateful=True,return_state=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state=tf.Variable(np.zeros((batch_size,embedding_vec_size)),trainable=False,name='Initial state')\n",
    "rnn_layer.initial_state=init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_2=rnn_layer(phase_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer=keras.layers.Dense(vocab_size)\n",
    "phase_3=dense_layer(phase_2)\n",
    "\n",
    "activation=keras.layers.Activation('softmax')\n",
    "phase_4=activation(phase_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground Truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['banknote',\n",
       " 'berlitz',\n",
       " 'calloway',\n",
       " 'centrust',\n",
       " 'cluett',\n",
       " 'fromstein',\n",
       " 'gitano',\n",
       " 'guterman',\n",
       " 'hydro-quebec',\n",
       " 'ipo',\n",
       " 'kia',\n",
       " 'memotec',\n",
       " 'mlx',\n",
       " 'nahb',\n",
       " 'punts',\n",
       " 'rake',\n",
       " 'regatta',\n",
       " 'rubens',\n",
       " 'sim',\n",
       " 'snack-food']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word(tar[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wholesale',\n",
       " 'upside',\n",
       " 'obtaining',\n",
       " 'artists',\n",
       " 'score',\n",
       " 'score',\n",
       " 'flies',\n",
       " 'flies',\n",
       " 'flies',\n",
       " 'flies',\n",
       " 'discontinued',\n",
       " 'innovative',\n",
       " 'innovative',\n",
       " 'recommended',\n",
       " 'pearce',\n",
       " 'pearce',\n",
       " 'trained',\n",
       " 'aroused',\n",
       " 'approve',\n",
       " 'approve']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word(np.argmax(phase_4[0,0:num_steps,:],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn=keras.Sequential()\n",
    "rnn.add(embedding_layer)\n",
    "rnn.add(rnn_layer)\n",
    "rnn.add(dense_layer)\n",
    "rnn.add(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (30, 20, 200)             2000000   \n",
      "_________________________________________________________________\n",
      "rnn (RNN)                    (30, 20, 128)             671088    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (30, 20, 10000)           1290000   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (30, 20, 10000)           0         \n",
      "=================================================================\n",
      "Total params: 3,961,088\n",
      "Trainable params: 3,955,088\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y,y_hat):\n",
    "    return keras.losses.sparse_categorical_crossentropy(y,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=184.20694>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=tf.reduce_sum(cross_entropy(tar,phase_4))/batch_size\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=tf.Variable(0.0,trainable=False)\n",
    "lr.assign(learning_rate)\n",
    "optimizer=keras.optimizers.SGD(lr,clipnorm=max_grad_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding/embeddings:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell/kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell/recurrent_kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell/bias:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell_1/kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell_1/recurrent_kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell_1/bias:0',\n",
       " 'dense/kernel:0',\n",
       " 'dense/bias:0']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=rnn.trainable_variables\n",
    "[var.name for var in rnn.trainable_variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y_hat=rnn(inp)\n",
    "    loss=cross_entropy(tar,y_hat)\n",
    "    cost=tf.reduce_sum(loss)/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clipping gradient values manually to see effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=68.83176>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads=tape.gradient(loss,var)\n",
    "clipped,init_norm=tf.clip_by_global_norm(grads,max_grad_norm)\n",
    "init_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying gradients to see effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.apply_gradients(zip(clipped,var))\n",
    "y_hat=rnn(inp)\n",
    "loss_1=cross_entropy(tar,y_hat)\n",
    "cost_1=tf.reduce_sum(loss_1)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cost:  tf.Tensor(184.20651, shape=(), dtype=float32)\n",
      "Cost after epoch=1:  tf.Tensor(172.78825, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('Initial cost: ',cost)\n",
    "print('Cost after epoch=1: ',cost_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As cost decreases model becomes more robust and its efficacy increases after certain epoch provided adequate data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
