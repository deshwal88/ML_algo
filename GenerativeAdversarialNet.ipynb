{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we generate mnist dataset examples by using Deep Convolutional Generative Adversarial network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow.keras as keras\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are gonna use tangent hyperbolic function(-1,1) in our generator so we shall scale our data to zero mean with limits(-1,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=mnist.load_data()\n",
    "x,y=train\n",
    "x=(x-127.5)/127.5\n",
    "x=np.expand_dims(x,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to display images in tiles form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(data,r,c,w=2,size=(8,8),batch=0):\n",
    "    \"\"\"\n",
    "    The function displays data points in tiles form.\n",
    "    \n",
    "    Args: \n",
    "        data: The tensor containing datapoints.\n",
    "        l: number of images to be arranged in rows.\n",
    "        b: number of images to be arranged in columns.\n",
    "        w: width of separator between images.\n",
    "        size: sigure size\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    img=iter(tf.constant(data))\n",
    "\n",
    "    for i in range(1,(r*c)+1):\n",
    "\n",
    "        if i%c==1:\n",
    "            arr=img.__next__().numpy()\n",
    "            continue\n",
    "\n",
    "        arr=np.append(arr,np.ones((28,w,1))*arr.max(),axis=1)\n",
    "        arr=np.concatenate([arr,img.__next__().numpy()],axis=1)\n",
    "\n",
    "        if i%c==0.0:\n",
    "            if i/c==1.0:\n",
    "                fig=arr\n",
    "            else:\n",
    "                fig=np.concatenate([fig,np.ones((w,fig.shape[1],1))*arr.max()],axis=0)\n",
    "                fig=np.concatenate([fig,arr],axis=0)\n",
    "\n",
    "    figure=plt.figure(figsize=size)\n",
    "    plt.imshow(fig,'gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    figure.savefig(\"mnist_%d.png\"%(batch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generator:- Generates images from random noise of a specific shape. This includes:-<br>\n",
    "<br>\n",
    ">*Convolutional transpose layer which is convolution in reverse direction, i.e. increasing strides increases size of output image thus no need to up sample. <br>\n",
    "*Batch normalization is used to normalize output from layers to a standard mean with beta=0.8 in order to make learning process stable.\n",
    "*Dropout layer is not used as its combination with batch normalization layer may cause unstablity.<br>\n",
    "*For activation we use Leaky Relu which gives a small value rather than zero when input is less than negative.<br>\n",
    "*At last we use tangent hyperbolic function as activation to give output as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise_shape):\n",
    "    inp=keras.layers.Input(noise_shape)\n",
    "    x=keras.layers.Dense(7*7*256,name='Dense_layer')(inp)\n",
    "    x=keras.layers.LeakyReLU()(x)\n",
    "    x=keras.layers.Reshape((7,7,256))(x)\n",
    "    assert tuple(x.shape)==(None, 7, 7, 256)\n",
    "    \n",
    "    x=keras.layers.Conv2DTranspose(128,kernel_size=(4,4),strides=(1,1),padding='same',\\\n",
    "                                   name='ConvT_layer_1')(x)\n",
    "    x=keras.layers.BatchNormalization(momentum=0.8)(x)\n",
    "    x=keras.layers.LeakyReLU()(x)\n",
    "    assert tuple(x.shape)==(None,7,7,128)\n",
    "    \n",
    "    x=keras.layers.Conv2DTranspose(64,kernel_size=(4,4),strides=(2,2),padding='same',\\\n",
    "                                  name='ConvT_layer_2')(x)\n",
    "    x=keras.layers.BatchNormalization(momentum=0.8)(x)\n",
    "    x=keras.layers.LeakyReLU()(x)\n",
    "    assert tuple(x.shape)==(None,14,14,64)\n",
    "    \n",
    "    x=keras.layers.Conv2DTranspose(1,kernel_size=(4,4),strides=(2,2),padding='same',\\\n",
    "                                  name='ConvT_layer_3')(x)\n",
    "    x=keras.layers.BatchNormalization(momentum=0.8)(x)\n",
    "    out=keras.layers.Activation('tanh')(x)\n",
    "    assert tuple(x.shape)==(None,28,28,1)\n",
    "    \n",
    "    return keras.models.Model(inp,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Discriminator:- This is a simple convolutional network which takes input as image and predicts fake or real using sigmoid as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(img_size):\n",
    "    inp=keras.layers.Input(img_size)\n",
    "    x=keras.layers.Conv2D(32,kernel_size=(5,5),strides=(1,1),padding='same')(inp)\n",
    "    x=keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x=keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x=keras.layers.Conv2D(64,kernel_size=(5,5),strides=(1,1),padding='same')(x)\n",
    "    x=keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x=keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x=keras.layers.Conv2D(128,kernel_size=(5,5),strides=(1,1),padding='same')(x)\n",
    "    x=keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x=keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x=keras.layers.Flatten()(x)\n",
    "    x=keras.layers.Dense(1)(x)\n",
    "    out=keras.layers.Activation('sigmoid')(x)\n",
    "    \n",
    "    return keras.models.Model(inp,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an instance of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer used is ADAM which not only adapts step size but works with momentum too thus results in stable learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While creating a combined model we initialize discriminater as non trainable and then compile to see the effect. Thus the generator and discrimator are trained separately.\n",
    "Where, loss is binary cross entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCGAN():\n",
    "    opt=keras.optimizers.Adam(lr=0.0002,beta_1=0.5)\n",
    "    dis=discriminator((28,28,1))\n",
    "    dis.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    gen=generator((100,))\n",
    "    inp=keras.layers.Input((100,))\n",
    "    x=gen(inp)\n",
    "    \n",
    "    dis.trainable=False\n",
    "    out=dis(x)\n",
    "    combined=keras.models.Model(inp,out)\n",
    "    combined.compile(optimizer=opt,loss='binary_crossentropy')\n",
    "    \n",
    "    return gen,dis,combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen,disc,combined=DCGAN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dis,comb,batch_size,batch_len,data):\n",
    "    w=int(np.sqrt(batch_size))\n",
    "    t0=time.time()\n",
    "    for i in range(1,batch_len+1):\n",
    "        t1=time.time()\n",
    "        inp=tf.random.normal(mean=0,stddev=1,shape=(batch_size,100,))\n",
    "        x_fake=gen(inp)\n",
    "        y_fake=tf.zeros((batch_size,))\n",
    "        \n",
    "        ind=np.random.randint(0,len(data),batch_size)\n",
    "        x_real=data[(ind)]\n",
    "        y_real=tf.ones((batch_size,))\n",
    "        \n",
    "        loss_fk=dis.train_on_batch(x_fake,y_fake)\n",
    "        loss_rl=dis.train_on_batch(x_real,y_real)\n",
    "        total_loss,acc=0.5*np.add(loss_fk,loss_rl)\n",
    "        \n",
    "        g_loss=comb.train_on_batch(inp,y_real)\n",
    "        t2=time.time()\n",
    "        if (i%100==0) and (i>1):\n",
    "            print(\"Batch: %d/%d [G-Loss: %.3f, D-Loss: %.3f, Accuracy: %.2f%%] [Batch time: %.3f sec, Total time: %.2f min]\\n\"\\\n",
    "                  %(i,batch_len,g_loss,total_loss,acc*100,t2-t1,(t2-t0)/60))\n",
    "            show_data(x_fake,w,w,size=(5,5),batch=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(disc,combined,16,batch_len=1000,data=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(disc,combined,32,batch_len=1000,data=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=gen(tf.random.normal((25,100)))\n",
    "show_data(im,5,5,w=2,size=(5,5),batch=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
